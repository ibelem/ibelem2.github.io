<!DOCTYPE html>
<html lang="{{ page.lang | default: site.lang | default: "en" }}">

{%- include head.html -%}

<body>

  {%- include header.html -%}

  <div class="wrapper faqs gr">

    <div class="box faq-webnn">
      <div class="cap">
        <p class="title">Web Neural Network API</p>
        <p class="subtitle">A dedicated low-level API for neural network inference hardware acceleration.</p>
      </div>
      <div class="accordion">
        <div class="accordion-item">
          <button id="accordion-button-1" aria-expanded="false"><span class="accordion-title">What’s all this
              then?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">


            <div class="post-content e-content" itemprop="articleBody">
              <p><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a> (ML) is a branch of <a
                  href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a>. A subfield
                of ML called <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> with its various
                neural network architectures enables new compelling user experiences for web applications. Use cases
                range from improved video conferencing to accessibility-improving features, with potential improved
                privacy over cloud-based solutions. Some common use cases that apply to a wide range of web applications
                include e.g.:</p>

              <ul>
                <li>Person detection</li>
                <li>Face detection</li>
                <li>Semantic segmentation</li>
                <li>Skeleton detection</li>
                <li>Style transfer</li>
                <li>Super resolution</li>
                <li>Image captioning</li>
                <li>Machine translation</li>
                <li>Noise suppression</li>
              </ul>

              <p>These use cases and more are elaborated in the <a
                  href="https://webmachinelearning.github.io/webnn/#usecases">use cases section of the API
                  specification</a>.</p>

              <p>While some of these use cases can be implemented in a constrained manner with existing Web APIs (e.g.
                WebGL graphics API, as demonstrated by the <a
                  href="https://github.com/webmachinelearning/webnn-polyfill">WebNN API polyfill</a> and the first-wave
                JS ML frameworks), the lack of access to platform capabilities beneficial for ML such as dedicated ML
                hardware accelerators constraints the scope of experiences and leads to inefficient implementations on
                modern hardware. This disadvantages the web platform in comparison to native platforms.</p>

              <p>The design process of the <a href="http://webmachinelearning.github.io/webnn/">Web Neural Network
                  API</a> (WebNN API) started by identifying the <a
                  href="https://webmachinelearning.github.io/webnn/#usecases">key use cases</a> working with the <a
                  href="https://www.w3.org/community/webmachinelearning/participants">diverse participants</a> including
                major browser vendors, key ML JS frameworks, interested hardware vendors, and web developers. After
                identification of the key use cases, the group worked down the levels of abstraction and <a
                  href="https://github.com/webmachinelearning/webnn/blob/master/op_compatibility/first_wave_models.md">decomposed
                  the key use cases into requirements</a>. The aim of this use case-driven design process was to put <a
                  href="https://w3ctag.github.io/design-principles/#priority-of-constituencies">user needs first</a>.
              </p>

              <p>With emerging ML innovations in both software and hardware ecosystem, one of the main challenges for
                the web is to bridge this software and hardware development and bring together a solution that scales
                across hardware platforms and works with any framework for web-based machine learning experiences. We
                propose the WebNN API as an abstraction for neural networks in the web browsers.</p>
            </div>
          </div>
        </div>

        <div class="accordion-item">
          <button id="accordion-button-2" aria-expanded="false"><span class="accordion-title">How is the architecture of
              WebNN?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>As illustrated in the architecture diagram of the figure above, web browsers may implement the WebNN API
              using native machine learning API available in the operating system. This architecture allows JavaScript
              frameworks to tap into cutting-edge machine learning innovations in the operating system and the hardware
              platform underneath it without being tied to platform-specific capabilities, bridging the gap between
              software and hardware through a hardware-agnostic abstraction layer.</p>
            <p><img src="/assets/images/faq/webnn_arch.png" alt="WebNN architecture"></p>
            <p>At the heart of neural networks is a computational graph of mathematical operations. These operations are
              the building blocks of modern machine learning technologies in computer vision, natural language
              processing, and robotics.</p>
          </div>
        </div>

        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What are the goals and
              non-goals of WebNN API?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <h3 id="goals">Goals</h3>

            <p>Web applications and frameworks can take advantage of the native operating system services for machine
              learning and the underlying hardware innovations available on the user’s computers to implement
              consistent, efficient, and reliable ML experiences on the web platform.</p>

            <h3 id="non-goals">Non-goals</h3>

            <ol>
              <li>
                <p>We do not define model serialization format. Formats are framework’s choices, which may be
                  vendor-specific. The role of the WebNN API is to facilitate solutions that work across the web
                  regardless of the model format by leveraging the underlying support available in the target platform
                  for reliable and efficient results.</p>
              </li>
              <li>
                <p>We do not define the packaging and delivery mechanism of machine learning models, such as the choice
                  of encryption and content protection.</p>
              </li>
              <li>
                <p>The machine learning model’s input may be of any media type, such as video streams, images, or audio
                  signals. We do not define new media types but relying on the existing web standards for media types
                  needed for respective scenarios.</p>
              </li>
            </ol>

          </div>
        </div>


        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Can I use WebNN API on PC
              desktop?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <h3 id="target-hardware">Target hardware</h3>

            <p>Web applications and frameworks can target typical computing devices on popular operating systems that
              people use in their daily lives. Initial prototypes demonstrate respectable performance on:</p>

            <ul>
              <li>Smartphones e.g. Google Pixel 3 or similar</li>
              <li>Laptops e.g. 13” MacBook Pro 2015 or similar</li>
            </ul>

            <p>The WebNN API is not tied to specific platforms and is implementable by existing major platform APIs,
              such as:</p>

            <ul>
              <li>Android Neural Networks API</li>
              <li>Windows DirectML API</li>
              <li>macOS/iOS ML Compute API</li>
            </ul>

            <p>Depending on the underlying hardware capabilities, these platform APIs may make use of CPU parallelism,
              general-purpose GPU, or dedicated hardware accelerators for machine learning. The WebNN API provides <a
                href="https://webmachinelearning.github.io/webnn/#usecase-perf-adapt">performance adaptation</a> options
              but remains hardware agnostic.</p>
          </div>
        </div>


        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Can we standardize on just
              a Model Loader API?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>A <a href="https://github.com/webmachinelearning/model-loader/blob/master/explainer.md">Model Loader
                API</a> loads a model from a specified URL and outputs a model object on which the caller can execute.
              It leaves all the responsibilities of loading and processing a neural network model to the web browsers
              and the underlying operating systems while offering the web developers a simple API surface, akin to an
              image loading API.</p>

            <p>Although this design approach has a clear benefit in its simplicity, it faces a challenge in defining a
              standard model format that works across the various web browsers and operating systems on the user’s
              devices. In shifting the focus of the design towards the model format, it creates an opportunity for more
              fragmentation in the way ML is consumed on the web and encourages silos of vendor-specific ecosystems
              around the particular model formats of choice. Much like in the early days of the image format wars, the
              web developers will likely have a more difficult time understanding which model formats would work on
              which combinations of the web browsers and operating systems that they’re targeting.</p>

            <p>By defining the WebNN API as a model format-agnostic set of neural network operations, we shift the focus
              of the design towards the abstraction between the web browsers and the underlying operating system
              services and let the web applications and JavaScript frameworks continue to focus on satisfying the needs
              of the web developers knowing that the neural networks they create will faithfully execute regardless of
              the browser’s underlying platform. What we believe works in our favor is the significant overlap of neural
              network operations and algorithms across all popular frameworks today. Models available in one format are
              generally convertible to another with little loss.</p>

            <p>A Model Loader API can also be built atop a Web Neural Network API without losing the appeal in its
              simplicity. Our view is that the two APIs are complementary and not mutually exclusive to each other;
              however we must start with the Web Neural Network API to ensure cross-platform interoperability, a
              cornerstone of the web platform.</p>

            <p>An explainer for the Model Loader API can be found <a
                href="https://github.com/webmachinelearning/model-loader/blob/master/explainer.md">here</a>.</p>

            <table>
              <thead>
                <tr>
                  <th>&nbsp;</th>
                  <th>Web Neural Network API</th>
                  <th>Model Loader API</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>API style</td>
                  <td>Graph builder (~low-level)</td>
                  <td>Model loader (~high-level)</td>
                </tr>
                <tr>
                  <td>Spec status</td>
                  <td>Advancing to WG</td>
                  <td>Explainer</td>
                </tr>
                <tr>
                  <td>Impl experience</td>
                  <td>Chromium POC, webnn-native</td>
                  <td>No</td>
                </tr>
                <tr>
                  <td>Key customer</td>
                  <td>JS ML frameworks</td>
                  <td>Web developers</td>
                </tr>
                <tr>
                  <td>Inference</td>
                  <td>Yes</td>
                  <td>Yes</td>
                </tr>
                <tr>
                  <td>Training</td>
                  <td>No, but possible in future</td>
                  <td>No</td>
                </tr>
                <tr>
                  <td>Polyfill</td>
                  <td>Yes</td>
                  <td>No, possible on top of WebNN API</td>
                </tr>
                <tr>
                  <td>Interop</td>
                  <td>Supported by major OS APIs</td>
                  <td>No, due to lack of standard model format</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What is the right level of
              abstraction for the neural network operations?</span><span class="icon"
              aria-hidden="true"></span></button>
          <div class="accordion-content">

            <p>Neural network operations are mathematical functions. There are about a hundred standard functions
              universally supported in popular frameworks today e.g. convolution, matrix multiplication, various
              reductions, and normalizations. Additionally, some frameworks provide an even more extensive set of
              variants of these functions for ease of use.</p>

            <p>In designing the WebNN operations, a proposal to decompose high-level functions to the more rudimentary
              mathematical operations was considered, with the key benefit of having a reduced number of operations
              defined. However, such an approach would make the networks more verbose and harder to construct. It’ll
              also risk losing the opportunity to leverage known optimizations for highly reusable functions in the
              operating systems and in the hardware platforms underneath it. For instance, most operating systems and
              modern hardware today support widely-used variants of convolutions and recurrent networks out of the box.
              By decomposing well-known functions into networks of rudimentary mathematical operations, their identities
              may be lost in the process with opportunities for significant performance gains left behind.</p>

            <p>To balance the needs of providing for future extensibility while ensuring maximum reuse and performance
              optimization opportunity, we chose to include both the standard functions and all the smaller operations
              making up the functions in the spec. For each high-level function defined, we make sure that all of its
              decomposed operations are also defined. This way, a newly-conceived function may be represented as a
              network of our decomposed operations, while a standard function can also be fully supported by the
              underlying platforms. An elaborate example of this principle is in the way we define the specification of
              the <a href="https://webmachinelearning.github.io/webnn/#api-modelbuilder-grucell">gruCell</a> operation
              as described in its notes.</p>

          </div>
        </div>

        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What alternatives have
              been
              considered?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <h3 id="stay-the-course-and-build-machine-learning-solutions-on-webglwebgpu">Stay the course and build
              machine learning solutions on WebGL/WebGPU</h3>

            <p>WebGL and WebGPU are Web API abstraction to the underlying graphics API, which could be used to implement
              neural network operations that run on the GPU. Popular JavaScript machine learning frameworks such as
              TensorFlow.js already uses WebGL and are working on a WebGPU backend. An alternative to the WebNN proposal
              is to continue with this architecture and rely on JavaScript frameworks implemented with these graphics
              abstraction to address the current and future needs of ML scenarios on the web.</p>

            <p>We believe this alternative is insufficient for two reasons. First, although graphics abstraction layers
              provide the flexibility of general programmability of the GPU graphics pipelines, they are unable to tap
              into hardware-specific optimizations and special instructions that are available to the operating system
              internals. The hardware ecosystem has been investing significantly in innovating in the ML space, and much
              of that is about improving the performance of intensive compute workloads in machine learning scenarios.
              Some key technologies that are important to model performance may not be uniformly accessible to
              applications through generic graphics pipeline states.</p>

            <p>Secondly, the hardware diversity with numerous driver generations make conformance testing of neural
              network operations at the framework level more challenging. Conformance testing, compatibility, and
              quality assurance of hardware results have been the traditional areas of strength of the operating
              systems, something that should be leveraged by frameworks and applications alike. Since neural network
              models could be used in mission-critical scenarios such as in healthcare or industry processes, the
              trustworthiness of the results produced by the frameworks are of utmost importance to the users.</p>

          </div>
        </div>

        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Why not define a model format that
            covers both topology and weights?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Defining a model format should simplify model loading, but it was explicitly stated as a non-goal for
              WebNN API to directly support model format as we believe formats should be left with the framework or the
              app using it.</p>
          </div>
        </div>

      </div>
    </div>

    <div class="box faq-modelloader">
      <div class="cap">
        <p class="title">Model Loader API</p>
        <p class="subtitle">An API to load a custom pre-trained machine learning model.</p>
      </div>
      <div class="accordion">
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What is Model Loader
              API?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <div class="post-content e-content" itemprop="articleBody">
              <p><strong>Model Loader</strong> is a proposed web API to load a custom, pre-trained machine learning
                model in a standard format, compile
                it for the available hardware, and apply it to example data in JavaScript in order to perform inference,
                like classification,
                regression, or ranking. The idea is to make it as easy as possible for web developers to use a custom,
                pre-built machine learning model in their web app, across devices and browsers.</p>

              <p>Performing inference locally can:</p>

              <ul>
                <li>Preserve privacy, by not shipping user data across the network</li>
                <li>Improve performance, by eliminating network latency</li>
                <li>Provide a fallback if network access is unavailable, possibly using a smaller and lower quality
                  model</li>
              </ul>

              <p>The API is modeled after existing inference APIs like TensorFlow Serving, Clipper, TensorRT, and MXNet
                Model Server, which
                are already widely used by many products and organizations for large volumes of requests. Using an API
                that’s similar to a
                server-based API makes it easier to switch between server-based and local-based inference.</p>

              <p>Unlike the Shape Detection API, the model loader APIs are generic. Application-specific libraries and
                APIs could be built on
                top.</p>

              <p>The API does not support training a model, modifying a model, federated learning, or other
                functionality. Maybe future
                APIs could address those, if useful.</p>

            </div>
          </div>
        </div>

        <!-- <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What does the sample
              code look like?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>The underlying implementation can use any hardware acceleration available on the device, as an internal
              implementation
              detail.</p>

            <h3 id="sample-code">Sample code</h3>

            <pre class="highlight"><code>const modelUrl = 'url/to/ml/model';
            var exampleList = [{
              'Feature1': value1,
              'Feature2': value2
            }];
            var options = { 
              maxResults = 5
            };
            
            const modelLoader = navigator.ml.createModelLoader();
            const model = await modelLoader.load(modelUrl)
            const compiledModel = await model.compile()
            compiledModel.predict(exampleList, options)
              .then(inferences =&gt; inferences.forEach(result =&gt; console.log(result)))
              .catch(e =&gt; {
                console.error("Inference failed: " + e);
              });
            </code></pre>
          </div>
        </div> -->
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">How do we know this level
              of API will be useful?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>This API proposal is modeled after existing services like TensorFlow Serving, Clipper, TensorRT, and
              MXNet Model Server
              which provide an API at the same level of abstraction. These existing APIs are already used by thousands
              of organizations
              large and small, serving billions of inference requests per day.</p>

          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Why not just use a machine
              learning library like brain.js or tensorflow.js?</span><span class="icon"
              aria-hidden="true"></span></button>
          <div class="accordion-content">

            <p>You totally can. In fact, those libraries could function as a polyfill for browsers that don’t yet
              support a standard
              model loader API.</p>

            <p>It’s possible that in the relatively near future, the operating system on every new phone, laptop,
              server, and other device
              will ship with a built-in ML engine that takes advantage of the underlying hardware. That ML inference
              software
              will have the support of the OS authors, will provide versioning, and will be heavily optimized. In that
              hypothetical future
              world, it wouldn’t make much sense to ship a duplicate copy of the same functionality, likely in a
              worse-performing
              implementation.</p>

            <p>The benefits of a built-in model loader API are:</p>

            <ul>
              <li>Friendlier developer experience.</li>
              <li>Operating system-level code can implement the API in C++, and the web can provide an API to access it.
                By contrast,
                JavaScript is slower than native code. Realistically, WASM is probably the alternative.</li>
              <li>Full access to hardware acceleration is available without the developer doing extra work or relying on
                a library that’s
                limited to WASM, WebGPU, or WebGL.</li>
              <li>Fewer bytes to ship.</li>
              <li>A standardized ML model format. ML models become as easy to use as images or media files.</li>
            </ul>

          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Why not just add more web
              standard APIs that are application-specific?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">

            <p>Application-specific APIs like face detection and barcode detection have some issues:</p>

            <ul>
              <li>Developers want to run custom models too. Application-specific APIs can cover the top use cases only,
                and with limited
                flexibility</li>
              <li>Organizations want to differentiate based on their data and their own trained models. They can’t with
                canned models.</li>
              <li>The APIs may give different results on different browsers, if the models themselves are provided with
                the browser and
                operating system.</li>
            </ul>

            <p>A generic inference API addresses these concerns. Something like the Shape Detection APIs could be built
              on top of ML
              Inference, with the benefit that developers could swap in their own models, and use those same models
              across browsers and
              devices.</p>

          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Why not use a graph API
              like the Neural Network API (NN API) on Android?</span><span class="icon"
              aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>There are a few reasons:</p>

            <ul>
              <li>Most web developers just want to perform inference on a pre-trained model. They don’t need to
                construct a model in
                JavaScript, so the web platform API surface doesn’t need to include all of those operations, and web
                developers wouldn’t
                use them directly.</li>
              <li>Only a small number of ML library authors would actually use a graph API directly. The library authors
                would in turn
                create the model loader API that web developers want.</li>
              <li>A graph API would have a very large API surface, and the operations in it are rapidly evolving. It’s
                easier to let a
                model format evolve than to codify it in JavaScript APIs.</li>
              <li>Building the complete graph representation into a JavaScript API means it can’t be used for
                proprietary models. The
                entire model can be accessed by JavaScript. That said, a DRM-like solution for protected models would be
                very hard.</li>
              <li>Standardizing at the graph level will further fragment the web, at the level where web developers
                write their application
                code, because they’re forced to rely on higher level ML libraries and abstractions. They’ll end up not
                as web developers
                writing with web standards, but as TensorFlow.js developers, or WinML developers, writing with
                framework-specific APIs.</li>
              <li>The Google team that has worked very closely with the NN API, and knows its history in great detail,
                does not recommend
                moving forward at this level of abstraction, and is actively working on alternatives like MLIR to
                address the pitfalls and
                shortcomings.</li>
            </ul>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Why does the TensorFlow
              team not recommend standardizing on a graph API?</span><span class="icon"
              aria-hidden="true"></span></button>
          <div class="accordion-content">

            <p>The short answer is that ML is evolving too quickly, and the graph and operation level of abstraction has
              been too unstable.
              Even with an operating system that a single company largely controls, on devices that have a relatively
              short lifespan, the
              graph model has not worked out. Given that the web requires compatibility for much longer timeframes, and
              is developed by
              a community, the TensorFlow team believes that what has not worked for Android is even less likely to work
              for the web.</p>

            <p>This same concern applies to model formats for a model loader API that are ML operation-based. In the
              time since
              TensorFlow launched, the number of operations has grown rapidly. The field of ML is rapidly changing, and
              new capabilities
              are approaches are published daily. Any graph or operation-based approach is going to face an ongoing
              challenge of
              standardizing and evolving. It’s not possible to design a great API, ship, and be done, and know that the
              API will endure.
              Its surface will need to be continually added to, and parts of it will fall into disuse as the field finds
              better approaches.
              This isn’t the type of API that lends itself to massive scale deployment with backwards compabibility
              guarantees.</p>

            <p>The Android and TensorFlow teams have had experience shipping an API to hundreds of millions of devices,
              supporting it,
              and evolving it. The fact that they have decided against tensor operations as the level of abstraction
              going forward ought to be a caution against following the same path for web standards.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What kinds of machine
              learning could a model loader API support?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>The idea is to support any type, such as image classification, binary classification, logistic
              regression, sequence models,
              or ranking.</p>

            <p>The API signature, inputs, and outputs would need to be carefully reviewed to make sure they work for all
              supported ML model
              types.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What are the requirements
              for the model format(s)?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Any supported model format must be:</p>

            <ul>
              <li>An open standard, without IP restrictions</li>
              <li>Versioned</li>
              <li>Backwards compatible (or else translatable)</li>
              <li>Vendor neutral</li>
            </ul>

            <p>In order for a model format to endure, it must also be:</p>

            <ul>
              <li>At the right level of abstraction</li>
            </ul>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What is the right level of
              abstraction for the model format?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>The model format could be:</p>

            <ol>
              <li>A set of tensor operations with a graph</li>
              <li>Hardware-specific instructions</li>
              <li>An intermediate representation</li>
            </ol>

            <p>Recommended, in the long-term: 3. A near to mid-term solution could be 1.</p>

            <p>The most widely used open-source formats are PyTorch’s and TensorFlow’s. There’s also Keras, which is
              supported by
              TensorFlow, Theano, and CNTK. Also of note is ONNX, which is intended to be convertible to and from other
              frameworks like
              PyTorch and TensorFlow. For all of these model formats, there are issues around backwards compatibility,
              and a translation
              layer may be required to ensure models continue to run in the future. All of these currently popular
              formats are based on
              the same premise: enumerating a set of high-level operations and encoding them, often in protocol
              buffers). This level
              of abstractionis exactly what the TensorFlow team recommends against, and is moving away from.</p>

            <p>Ultimately, performance will depend upon hardware-specific instructions. But that’s not the right level
              of abstraction for
              the web, since the web platform needs to be hardware-independent.</p>

            <p>An intermediate representation has the potential to address the limitations of operation sets and
              hardware-specific
              instructions. One example of an intermediate representations is
              MLIR (multi-layer intermediate representation)](https://mlir.llvm.org/), which is part of the LLVM
              foundation.</p>

            <p>Potential benefits:</p>

            <ul>
              <li>Extensible: New operations can be compiled into the same intermediate representation, without changes
                to the spec</li>
              <li>Performant: Because it’s closer to the metal, it can perform better than operation sets.</li>
              <li>Custom ops can be defined in the intermediate representation, instead of JavaScript</li>
            </ul>

            <p>Note that a graph API could be built using MLIR, and JavaScript APIs for all of the primitives could be
              defined. This would
              satisfy the web vision of exposing browser internals at the lowest level.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Why not adopt MLIR
              today?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>If an intermediate representation like MLIR is the right approach for the web, why not standardize on it
              today?</p>

            <ul>
              <li>It isn’t ready. It’s very much a work in progress.</li>
              <li>Proof-of-concept support exists for TensorFlow, PyTorch, and ONNX using XLA HLO, but ‘s not
                production-ready.</li>
            </ul>

            <p>These are pretty serious barriers. MLIR could be the right long-term solution, but it can’t be adopted
              today.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Is a model loader API
              blocked on MLIR becoming a standard?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Not necessarily. It’s possible that the shape of the API could be standardized on, without the details of
              an IR being
              resolved yet. The community group could proceed to implement using an existing operation-level
              representation, like ONNX
              or TFlite. Perhaps by the time the browser vendors are ready to ship, an IR will be ready.</p>

            <p>Alternatively, browsers could ship an operation-based format sooner, and an IR later. The operation-based
              format could
              be experimental, and never ship without a flag. Or the web platform could support both kinds of format,
              indefinitely,
              and let developers choose which to use. Over time, the IR may become more popular.</p>

            <p>Options:</p>

            <p>A. Wait for an IR to become a standard before shipping anything; ship only an IR format.
              B. Ship an experimental API based on an operation-level format. Wait for IR before shipping to production.
              C. Ship an operation-based format, and later deprecate it and support an IR instead.
              D. Ship an operation-based format, and later add a second format that is an IR.</p>

            <p>Recommended: B or D?</p>

            <p>For either level of abstraction, whether an operation-based format, or an IR, the web could standardize
              on:</p>

            <ol>
              <li>A single canonical model format, used on all devices by all browsers.</li>
              <li>A single canonical model format, supported by all devices, and optional additional formats, that may
                not be supported on
                any given device.</li>
              <li>A set of permissible model formats, with at least one format guaranteed to be available, but that
                format might be
                different per device. In this option, all browsers running on a given device could use the same format,
                accessing the
                same underlying inference engine.</li>
              <li>A set of permissible model formats, all optional. Each device supports zero or more formats.</li>
            </ol>

            <p>Recommended: Long-term, option 1 is the ideal. Near to mid-term, 2 unblocks developers and OS and ML
              framework authors.</p>

            <p>In principle, it would be great to standardize on a single format, and a single level of abstraction.
              If a single canonical model format is adopted at a given level of abstraction, browser vendors could
              optionally
              support additional formats. There are multiple reasons they might want to do this:</p>

            <ul>
              <li>As a way to try out new functionality not yet supported in the standard</li>
              <li>For performance, to avoid a conversion step from the interop format to the internally used format, if
                the conversion is
                expensive.</li>
              <li>As a hedge in case the standard interoperable format doesn’t evolve quickly enough</li>
            </ul>

            <p>In principle, an IR would address these concerns, and they would mostly only apply to higher-level
              operation-based APIs,
              due to the rapid evolution of ML and need for new operations.</p>

            <p>If multiple formats are supported, web developers will have a more complicated time, but maybe it’s not
              totally horrible.
              Developers already serve different images for different screen resolutions. It might not be such a burden
              to ship different
              ML models for different devices, if the tooling exists to create those different models. In practice,
              developers may need
              to ship different models per device anyway due to differences in memory and CPU. For example, they might
              ship a smaller
              model on a mobile device, and a larger, more accurate model for desktop. As long as the model format is
              browser-independent,
              it might be ok. In other words, it might be ok if a web site needs to serve ONNX for Windows desktop,
              CoreML for iOS, and
              TFlite for Android, as long as those models work with the same API in Edge, Firefox, Safari, and Chrome,
              and there’s
              an easy enough way to create the models in the necessary formats.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">How can web developers
              protect their proprietary ML models?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Short answer: they can’t, under this initial proposal.</p>

            <p>But eventually this may need to be solved somehow. Data is often proprietary, and companies and
              organizations consider the ML
              model to be IP. For server-based inference, the model never needs to leave the remote server, which
              provides at least
              some level of protection.</p>

            <p>For OS vendors, there could be a way to specify local URLs that cannot be read from JavaScript. Eg, a URL
              scheme like ml://.
              There may well be other solutions. Creating a new URL scheme is not very appealing.</p>

            <p>For web applications, maybe there’s a more DRM-like solution. Maybe the browser could have an encrypted
              model download space
              that’s write-only, and it could fetch the remotely stored model over a secure connection. Once downloaded,
              the model could be
              referenced, but no JavaScript library could read or access the contents. Only the internal implementation
              code could access
              the model directly.</p>

            <p>This requires further thought, by experts.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">How would a browser vendor
              actually implement the API?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Naively, the hope is that the implementation would be a thin wrapper over native code that runs on the
              operating
              system. For example, the underlying ML implementation could be provided by WinML on Windows, the NN API or
              future MLIR API
              on Android, CoreML on iOS, and ML Service in Chrome OS. The browser development team would not need much
              ML expertise in
              order to implement the API.</p>

            <p>After discussions with browser experts, it’s clear that the browser needs to deeply understand the model
              and its
              internals, in order to avoid executing untrusted code. Only after parsing and validating thoroughly would
              a handoff to
              a lower-level compiler or sandboxed execution engine be safe.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">How will backwards
              compatibility be maintained?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>In this proposal, it’s up to the model format to ensure compatibility.</p>

            <p>Maybe backwards compatibility isn’t as important as we generall assume? ML is evolving quickly, and newer
              ML models tend to
              render older ones obsolete very quickly.</p>

            <p>Also, retraining often is important for some applications in order to maintain model freshness. For
              example, any rankings
              that consider recency or trends will need to be based on fresh models. If developers update their models
              often, there
              wouldn’t be a need to support old model formats, beyond the most recent version or two. It could even be
              built into the spec
              that no model older than, eg, 1 year is guaranteed to work.</p>

            <p>Even with frequent retraining, the model format needs to be versioned and there needs to be a way to
              maintain compatibility
              across subsequent releases. Implementations have to deal with parsing the model format and converting as
              needed.</p>

            <p>If the model format isn’t stable, and conversion between model formats is impossible or slow, backwards
              compatibility could
              require bundling multiple ML runtime versions. That could lead to bloat. Of course, with client-side
              solutions, the ML
              runtime is loaded separately for every site, so there’s even more potential bloat.</p>

            <p>Another idea is to make sure that there are polyfills, and set the expectation that the API will be
              versioned, and older
              versions will be deprecated and removed.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">ML is evolving quickly.
              How will this stay up to date?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Device hardware is diverse. Operating systems are varied. Machine learning implementations are changing.
              ML models are
              changing even faster, because new data is constantly becoming available. It’s a reality that things will
              change, and the
              underlying code will be different for different end users. The goal is to give access to hardware
              acceleration where
              available, and to minimize the burden of using custom ML models.</p>

            <p>The challenge is that the ML model format holds the complexity. Forwards compatibility is an issue, since
              models may be
              trained with newer versions of the ML frameworks, and browsers may lag several weeks or more behind.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What about sandboxing and
              isolation of the executed models?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Does allowing arbitrary ML models to be executed open up major security holes? How will isolation be
              handled?</p>

            <p>Currently, a common approach for native mobile apps is to bundle an ML runtime, like TF lite or CoreML,
              with each installed
              app. Because there’s an app review step for the app stores, there’s an extra layer of review. Security
              isolation is provided
              by the operating system.</p>

            <p>For the web, it’s a little trickier, since a single web browser installation manages multiple web apps.
              Isolation is handled
              by the browser. With the browser calling out to a shared installation of an ML runtime, that runtime would
              need to provide
              isolation between callers. Also, the browser would need to parse and validate the model in depth.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">What about fingerprinting?
              Doesn’t this make it easier?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>Yes, potentially. Because the API will be available or not, or execute much faster or slower, depending
              on the device,
              operating system, and version, more information will be available for fingerprinting and tracking of
              users.</p>

            <p>Eventually, over time, ML hardware will become more common, and the fingerprinting risk will diminish, so
              that perhaps it
              won’t be notably worse than for today’s GPUs and CPUs.</p>

            <p>In the meantime, it might be possible to bucket the API behavior into fewer categories, so there’s less
              information available
              for fingerprinting.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button id="accordion-button-4" aria-expanded="false"><span class="accordion-title">Is it too high level to
              ever get agreement and ship?</span><span class="icon" aria-hidden="true"></span></button>
          <div class="accordion-content">
            <p>A trend in web standards is to focus on small, low level APIs that are primarily used internally by
              JavaScript libraries and
              frameworks, which in turn provide higher level conveniences to web developers. An argument made in support
              of the low level
              focus is that the low level APIs unlock the higher level capabilities. Those higher level layers are often
              heavily focused
              on ergonomics and stylistic preferences, and it’s better to let client libraries figure those things out.
            </p>

            <p>An model loader API is lower level than the Shape Detection APIs, and higher level than WebGL and WebGPU.
            </p>

            <p>Web developers do not typically produce their own ML models. They receive models, created by a data
              scientist, and add them
              to a web app. It’s analogous to how web developers receive images created by a graphic designer and add
              them to a page. The
              model loader API is a low-level, generic API, like an image tag. The web platform could have evolved
              without an image tag,
              and instead offered only a canvas and a byte array input stream reader. But then web developers would be
              dependent on image
              rendering libraries built on top of those primitives, each with its own supported image formats, buffering
              behavior, and
              rendering. With access to a standard image tag, and standard image formats supported in all the browsers
              (even though not
              prescribed by the spec!), developers and hobbyists were able to create content easily. Maybe ML model
              loading is a similarly
              fundamental building block for the future of the intelligent web.</p>

            <p>The existence of widely adopted model serving APIs like TensorFlow Serving suggests that an API at this
              level of abstraction
              is broadly useful and can be standardized.</p>

            <p>The internals of a model loader API could also stabilize to the point where exposing the low-level
              instructions or operations
              directly to web developers makes sense.</p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="wrapper faqs mt">
    <p>This FAQ builds upon the explainer
      documents and as such may not always reflect the latest W3C group consensus.
      Please see the specifications and their GitHub repositories for the latest
      developments and discussions around these questions.</p>
    <p>Thanks to all the <a href="https://www.w3.org/community/webmachinelearning/">Machine Learning for the Web
        Community Group</a> and <a href="https://www.w3.org/2020/06/machine-learning-workshop/">W3C Workshop on Web and
        Machine Learning</a> participants for their comments and feedbacks.</p>
  </div>

  {%- include footer.html -%}

  {%- include script-faq.html -%}

</body>

</html>